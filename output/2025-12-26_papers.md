# HF Daily Papers - 2025-12-26

Total: 7 papers

---

## 1. Latent Implicit Visual Reasoning

**Upvotes**: 51 | **Authors**: Kelvin Li, Chuyi Shang, Leonid Karlinsky...

[HuggingFace](https://huggingface.co/papers/2512.21218) | [arXiv](https://arxiv.org/abs/2512.21218)

**输入→输出**: 视觉问答。输入: 图像和文本。输出: 文本。Backbone: Qwen2.5-VL-3B-Instruct、Qwen3-VL-4B-Instruct和LLaVA-OneVision-1.5-4B-Instruct。数据集: PixMo-Count, COCO, ArtBench-10, SPair-71k, HPatches, FunK-Point, MID 和 DreamSim。

**问题**: 现有的大型多模态模型(LMMs)主要以文本为中心，严重依赖语言作为核心的推理方式，导致它们在处理主要以视觉为主的推理任务时能力有限。同时，以往方法中使用辅助图像、深度图或图像裁剪等方式来监督中间视觉步骤，会对模型学习产生限制性先验，增加标注成本，并且难以跨任务泛化。

**方案**: 1. 模型结构：在LMM中加入隐式视觉推理token。2. 训练：使用负对数似然(NLL)损失函数，训练分为两个阶段：第一阶段冻结视觉编码器和投影层，训练隐式视觉推理token，answer tokens只能attend到prompt token和latent token；第二阶段解冻模型所有参数，token可以attend到图像token和latent token。使用AdamW优化器，学习率为1×10-4，权重衰减为0.01，beta为(0.9, 0.999)，epsilon为10-8。使用LoRA对注意力块和MLP块进行参数高效微调，LoRA秩r = 16，alpha = 32，dropout为0.05。3. 推理流程：输入图像和文本，模型通过隐式视觉推理token进行视觉推理，最终输出答案。4. 实验结果：在九个以视觉为中心的任务中，LIVR始终优于直接SFT。

<details>
<summary>Original Abstract</summary>

While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>

---

## 2. Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

**Upvotes**: 46 | **Authors**: Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel...

[HuggingFace](https://huggingface.co/papers/2512.20605) | [arXiv](https://arxiv.org/abs/2512.20605)

**输入→输出**: 任务: 自回归模型中的分层强化学习。输入: 观察和行动序列。输出: 分层策略。

**问题**: 仅依靠token级的变化进行探索可能不足以在需要生成多个正确token才能获得奖励的困难、稀疏奖励问题上取得进展。

**方案**: 1. 模型结构: 引入一个高阶非因果序列模型，其输出控制基本自回归模型的残差流激活。使用变分推断学习内部控制器。2. 训练: 自监督预训练，然后进行内部强化学习，直接强化内部激活。3. 推理流程: 通过组合多个控制器随时间推移来有效地探索新任务。4. 实验结果: 在稀疏奖励情况下，内部强化学习优于标准强化学习微调。

<details>
<summary>Original Abstract</summary>

Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>

---

## 3. Spatia: Video Generation with Updatable Spatial Memory

**Upvotes**: 20 | **Authors**: Jinjing Zhao, Fangyun Wei, Zhening Liu...

[HuggingFace](https://huggingface.co/papers/2512.15716) | [arXiv](https://arxiv.org/abs/2512.15716)

**输入→输出**: 任务: 视频生成。输入: 初始图像, 文本描述, 相机轨迹。输出: 一段视频。Backbone: Wan2.2 (包含 5B 参数)。数据集: RealEstate (40K 视频) 和 SpatialVID (HD 子集, 10K 视频)。

**问题**: 现有的视频生成模型难以维持长时间的空间和时间一致性，主要是因为视频信号的密集性和高维度特性。

**方案**: 1. 模型结构: Spatia 包含 8 个网络块, 每个网络块包含一个 ControlNet 块，与四个主要块并行运行. 每个主要块遵循 Wan2.2 的设计, 包含一个自注意力层, 一个交叉注意力层和一个 FFN。ControlNet 块采用相同的架构，但在 FFN 之后附加一个投影器（实现为一个简单的 MLP 层）。
2. 训练: 首先训练 ControlNet 块 8,000 次迭代, 同时冻结主网络。接下来, 冻结 ControlNet 块, 并使用 LoRA (rank = 64) 微调主块 5,000 次迭代。两个阶段都采用 AdamW 优化器, 学习率分别为 le-5 和 1e-4, 批量大小为 64, 在 64× AMD MI250 GPU 上进行。
3. 推理流程: 迭代进行, 每一次迭代，用户指定一个文本指令和一个基于当前3D场景点云的相机轨迹，以生成一个新的视频片段。然后,将新生成的内容与先前生成的片段一起，用于更新空间记忆（场景点云）.
4. 实验结果: 在 WorldScore 基准测试中, Spatia 在静态和动态世界得分方面优于其他模型。在 RealEstate 数据集中, Spatia 的 PSNR 为 18.58, SSIM 为 0.646, LPIPS 为 0.254。

<details>
<summary>Original Abstract</summary>

Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.

</details>

---

## 4. Schoenfeld's Anatomy of Mathematical Reasoning by Language Models

**Upvotes**: 12 | **Authors**: Ming Li, Chenrui Fan, Yize Cheng...

[HuggingFace](https://huggingface.co/papers/2512.19995) | [arXiv](https://arxiv.org/abs/2512.19995)

**输入→输出**: 任务: 分析大型语言模型解决数学问题的推理过程。输入: 410,991句由15个模型解决Omni-MATH子集的100个问题生成的句子。输出: 这些句子的功能性推理步骤（分析、探索、实施、验证等）。Backbone: 各种大型语言模型（GPT-4.1, GPT-5, Gemini-2.5-Flash, Gemini-2.5-Pro等等）。数据集: Omni-MATH的一个子集，包含100个数学问题。

**问题**: 大型语言模型推理迹的底层认知结构和步骤难以识别和分析，难以超越表面统计。

**方案**: 1. 模型结构: 使用Schoenfeld的Episode理论，将推理迹抽象为功能性推理步骤，例如分析、探索、实施、验证等。2. 训练: 采用sentence-level的标注方式，构建了7,067句人工验证的黄金数据集，并选择GPT-5作为自动标注器进行大规模标注。3. 推理流程: 通过ThinkARM框架进行自动化标注。4. 实验结果: 揭示了可重现的思维动态和推理模型与非推理模型之间的结构差异。

<details>
<summary>Original Abstract</summary>

Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.

</details>

---

## 5. How Much 3D Do Video Foundation Models Encode?

**Upvotes**: 7 | **Authors**: Zixuan Huang, Xiang Li, Zhaoyang Lv...

[HuggingFace](https://huggingface.co/papers/2512.19949) | [arXiv](https://arxiv.org/abs/2512.19949)

**输入→输出**: 任务: 从视频中进行3D重建，评估视频基础模型的3D感知能力。输入: 视频帧的特征。输出: 3D点云、深度图和相机姿态。Backbone: 浅层VGGT类Transformer。数据集: CO3Dv2和DL3DV。

**问题**: 之前的研究评估图像模型使用深度和跨视图一致性或使用离线的初始化进行单场景优化，缺乏对预训练视频模型中全局一致3D属性的直接探测。

**方案**: 1. 模型结构: 探针模型是一个浅层VGGT类的Transformer，包含四个交替注意层和三个读出头：两个用于3D点云和深度图的密集预测头，以及一个相机头。2. 训练: 使用类似于VGGT的多任务目标函数训练探针，包括点云图损失、深度损失和相机损失。3. 推理流程: 从视频基础模型中提取时空特征，然后使用训练好的探针模型预测3D点云、深度图和相机姿态。4. 实验结果: 在CO3Dv2上，WAN2.1-14B的点云误差为0.284，深度误差为0.151，AUC@30为0.736；在DL3DV上，WAN2.1-14B的点云误差为1.051，深度误差为0.323，AUC@30为0.660。

<details>
<summary>Original Abstract</summary>

Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.

</details>

---

## 6. VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation

**Upvotes**: 6 | **Authors**: Xinyao Liao, Qiyuan He, Kai Xu...

[HuggingFace](https://huggingface.co/papers/2512.19680) | [arXiv](https://arxiv.org/abs/2512.19680)

**输入→输出**: 任务: 类条件图像生成、文本条件图像生成。输入: 类别、文本。输出: 图像。Backbone: LlamaGen、Janus-Pro 1B。数据集: ImageNet-1K、LAION-COCO、Flux-Reason。

**问题**: 现有的自回归视觉生成模型在token级别进行优化，缺乏像素级别的监督，导致生成的图像质量下降，产生视觉伪影。

**方案**: 1. 模型结构: 提出VA-π框架，通过变分策略对齐，将像素重建和自回归建模统一起来。2. 训练: 使用ImageNet-1K的1%数据进行25分钟的微调，使用8个A100 GPU，不依赖外部奖励模型或自由运行采样，使用 AdamW 优化器，学习率为 1 × 10-6，权重衰减为 1 × 10-4。3. 推理流程: 在推理过程中，通过策略网络顺序生成视觉token。4. 实验结果: 在LlamaGen-XXL上，FID从14.36降低到7.65，IS从86.55提高到116.70，在GenEval上，LlamaGen从0.306提高到0.339，Janus-Pro从0.725提高到0.744。

<details>
<summary>Original Abstract</summary>

Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-π, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-π formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-π introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-π enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.

</details>

---

## 7. GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training

**Upvotes**: 3 | **Authors**: Tong Wei, Yijun Yang, Changhao Zhang...

[HuggingFace](https://huggingface.co/papers/2512.13043) | [arXiv](https://arxiv.org/abs/2512.13043)

**输入→输出**: 任务: 多轮视觉语言模型（VLM）智能体训练。输入: 视觉观察和文本指令。输出: 动作序列。Backbone: Qwen2.5-VL-7B, Qwen3-VL-8B。数据集: Points24, ALFWorld。

**问题**: 多模态智能体的强化学习（RL）训练受到稀疏奖励和长程信用分配的阻碍。现有方法通过查询教师模型来提供步级反馈，但依赖于昂贵且通常是特权的模型，限制了实用性和可重复性。

**方案**: 1. 模型结构: GTR-Turbo 通过合并在 RL 训练过程中产生的检查点权重，然后将此合并模型用作“免费”教师，以指导后续的 RL 训练。2. 训练: 采用 PPO 算法进行训练，训练过程中使用 SFT loss 或 KL 散度进行引导。Points24 训练 30000 步，ALFWorld 训练 20000 步，学习率使用 CosineAnnealingLR，初始学习率为 1e-5，最终学习率为 1e-9。 3. 推理流程:智能体基于观察生成想法和动作，教师模型生成参考想法，通过 SFT 或 KL 散度进行知识提炼。4. 实验结果: 在 Points24 任务上，GTR-Turbo 相对于 GTR，训练时间减少 50%，计算成本降低 60%，准确率提高 10-30%。

<details>
<summary>Original Abstract</summary>

Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>

---

