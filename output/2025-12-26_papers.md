# HF Daily Papers - 2025-12-26

Total: 3 papers

---

## 1. Latent Implicit Visual Reasoning

**Upvotes**: 51 | **Authors**: Kelvin Li, Chuyi Shang, Leonid Karlinsky...

[HuggingFace](https://huggingface.co/papers/2512.21218) | [arXiv](https://arxiv.org/abs/2512.21218)

**输入→输出**: 想象一下，输入是一张图片，比如一个复杂的电路图，然后你问AI：“这个电路如果我把这个元件拔掉，会发生什么？” 输出就是AI告诉你后果，比如：“电路会停止工作”。

**问题**: 以前的大型AI模型，虽然很厉害，但它们主要靠文字来理解世界，对图像的理解不够深入。 就像让一个书呆子只看文字描述来修电脑，而不是直接看电路图，很多视觉相关的任务就搞不定了。

**方案**: 这个新方法就像给AI配了一副“透视眼镜”。 它让AI自己学习图像中哪些部分重要，然后把这些重要部分“提取”出来，像是在图片上做了标注一样。这样AI就能更好地理解图像，解决那些需要视觉推理的问题了，而且不需要人工提前告诉它哪些部分重要。

<details>
<summary>Original Abstract</summary>

While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>

---

## 2. Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

**Upvotes**: 46 | **Authors**: Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel...

[HuggingFace](https://huggingface.co/papers/2512.20605) | [arXiv](https://arxiv.org/abs/2512.20605)

**输入→输出**: 想象你玩一个游戏，比如超级玛丽。普通的方法是，你每次按一个按钮（输入），玛丽就走一步（输出）。这个论文里的模型也是这样，输入一个“下一步”，模型就输出一个动作。

**问题**: 之前，这些模型只能一步一步地探索世界，就像让玛丽一下一下地跳，效率很低，尤其是在奖励很少的时候，比如要走很远才能找到一个金币。 这就导致模型很难学会复杂的策略，就像让玛丽学会跳过一个很长的坑。

**方案**: 这个论文的核心想法是，在模型内部建立一个“遥控器”，可以一次控制玛丽做好几个动作，比如“向前跑并跳跃”。 这个“遥控器”通过调整模型内部的参数来实现，然后直接训练这些“遥控器”来完成任务，这样玛丽就可以更高效地探索和学习新技能了。

<details>
<summary>Original Abstract</summary>

Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>

---

## 3. Spatia: Video Generation with Updatable Spatial Memory

**Upvotes**: 20 | **Authors**: Jinjing Zhao, Fangyun Wei, Zhening Liu...

[HuggingFace](https://huggingface.co/papers/2512.15716) | [arXiv](https://arxiv.org/abs/2512.15716)

**输入→输出**: 想象一下，你给电脑一个粗略的3D场景地图，然后告诉它"生成一段围绕这座房子的视频"，或者"加入一只猫到公园里"。Spatia 就能输出符合要求的视频片段，并且还能根据你的新指令持续修改和更新这个视频，同时保持场景的整体空间一致性。

**问题**: 以前的视频生成模型容易忘记场景中的物体位置和外观，导致视频一会儿清晰一会儿模糊，或者物体突然消失。这就好像你让朋友根据你的描述画一幅画，画到一半朋友忘记了房子具体的样子，画出来的房子就变得歪歪扭扭，甚至完全变了样， 这对于创作需要长时间保持场景连贯性的视频来说是个大问题。

**方案**: Spatia的核心想法是让电脑记住一个场景的3D结构，就像给它一张地图一样。它通过一种叫做SLAM的技术，像人一样用眼睛观察并绘制场景， 然后生成符合地图结构的视频，并且可以不断地更新这个地图，这样视频中的物体就不会乱跑乱变，你可以像玩3D游戏一样控制相机视角，并且对视频内容进行3D编辑。

<details>
<summary>Original Abstract</summary>

Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.

</details>

---

