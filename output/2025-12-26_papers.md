# HF Daily Papers - 2025-12-26

Total: 3 papers

---

## 1. Latent Implicit Visual Reasoning

**Upvotes**: 51 | **Authors**: Kelvin Li, Chuyi Shang, Leonid Karlinsky...

[HuggingFace](https://huggingface.co/papers/2512.21218) | [arXiv](https://arxiv.org/abs/2512.21218)

**输入→输出**: 该论文针对文本+图像输入，输出为文本的任务进行研究，旨在提升大型多模态模型 (LMMs) 在视觉推理任务上的性能，并且在没有明确视觉监督的情况下进行。

**问题**: 现有的 LMMs 主要以文本为中心进行推理，在以视觉为主导的推理任务中存在局限性。以往方法依赖辅助图像、深度图或图像裁剪等方式来监督中间视觉步骤，但这些方法对“有用”的视觉抽象施加了约束性的先验，增加了标注成本，并且难以跨任务泛化。

**方案**: 该论文提出了一种任务无关的机制，训练 LMMs 去发现和使用视觉推理标记（Visual Reasoning Tokens），无需显式监督。这些标记以全局方式关注图像，并以任务自适应的方式重新编码图像，从而使模型能够提取相关的视觉信息，而无需手动设计的监督。该方法优于直接微调，并在各种以视觉为中心的任务上取得了最先进的结果，并且能够泛化到多任务指令微调。

<details>
<summary>Original Abstract</summary>

While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>

---

## 2. Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

**Upvotes**: 46 | **Authors**: Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel...

[HuggingFace](https://huggingface.co/papers/2512.20605) | [arXiv](https://arxiv.org/abs/2512.20605)

**输入→输出**: 论文研究的是如何利用自回归模型进行强化学习 (RL)。输入是状态信息，输出是动作序列。

**问题**: 传统的自回归模型在强化学习微调时，通过逐个token采样生成动作，导致探索效率低下，尤其是在奖励稀疏的环境中。之前的模型难以有效地学习具有时间抽象概念的动作。

**方案**: 该论文提出了一种新的强化学习方法，称为“内部强化学习”(Internal RL)。它引入了一个高阶非因果序列模型，该模型控制基础自回归模型的残差流激活。高阶模型学习将长序列激活压缩到内部控制器中，每个控制器执行一系列行为上有意义的动作，并伴随学习到的终止条件。 通过对内部控制器进行直接强化，该方法能够在标准RL微调失败的情况下，从稀疏奖励中学习。实验结果表明，该方法在网格世界和基于MuJoCo的任务中表现出优势，特别是在具有分层结构的任务中，能够实现高效的探索。

<details>
<summary>Original Abstract</summary>

Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>

---

## 3. Spatia: Video Generation with Updatable Spatial Memory

**Upvotes**: 20 | **Authors**: Jinjing Zhao, Fangyun Wei, Zhening Liu...

[HuggingFace](https://huggingface.co/papers/2512.15716) | [arXiv](https://arxiv.org/abs/2512.15716)

**输入→输出**: 输入是文本提示和/或初始图像，输出是视频序列。论文中可能使用了未明确说明的通用视频生成数据集。

**问题**: 现有的视频生成模型难以维持长期的空间和时间一致性，主要原因是视频信号维度高且稠密。这导致生成视频中的物体形变、场景不连贯等问题。

**方案**: 本文提出了一种名为Spatia的视频生成框架，该框架通过显式地维护一个3D场景点云作为持久化的空间记忆来解决上述问题。Spatia迭代地生成以空间记忆为条件的视频片段，并通过视觉SLAM技术持续更新该空间记忆。这种动态-静态解耦的设计增强了生成过程中的空间一致性，并保留了模型生成逼真动态实体的能力。此外，Spatia支持显式的相机控制和3D感知的交互式编辑。

<details>
<summary>Original Abstract</summary>

Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.

</details>

---

