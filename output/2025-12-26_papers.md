# HF Daily Papers - 2025-12-26

Total: 1 papers

---

## 1. Latent Implicit Visual Reasoning

**Upvotes**: 51 | **Authors**: Kelvin Li, Chuyi Shang, Leonid Karlinsky...

[HuggingFace](https://huggingface.co/papers/2512.21218) | [arXiv](https://arxiv.org/abs/2512.21218)

**输入→输出**: 输入是图像和文本，输出是模型理解图像内容的能力，比如识别物体或场景。例子是让模型识别图片中的动物。

**问题**: 之前的模型主要依赖文本，处理视觉任务时效果差，比如识别复杂图像时容易出错。这很重要，因为很多任务需要视觉理解。

**方案**: 他们开发了一种新机制，让模型在没有明确指导的情况下，自主发现和使用视觉信息。这样模型能更灵活地处理各种视觉任务。

<details>
<summary>Original Abstract</summary>

While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>

---

