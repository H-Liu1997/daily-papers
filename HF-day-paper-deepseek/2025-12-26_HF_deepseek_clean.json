[
  {
    "title": "Latent Implicit Visual Reasoning",
    "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.",
    "translation": "标题：潜在隐式视觉推理\n\n摘要：尽管大型多模态模型（LMMs）取得了显著进展，但它们仍然主要以文本为中心，依赖语言作为核心推理方式。因此，它们在处理以视觉为主的推理任务时能力有限。近期的方法试图通过使用辅助图像、深度图或图像裁剪来监督中间视觉步骤。然而，这些策略对“有用”视觉抽象的定义施加了限制性先验，增加了繁重的标注成本，并难以在任务之间进行泛化。为了解决这一关键限制，我们提出了一种与任务无关的机制，训练LMMs在没有显式监督的情况下发现和使用视觉推理标记。这些标记以全局方式关注并以任务自适应的方式重新编码图像，使模型能够提取相关的视觉信息，而无需手工制作的监督。我们的方法超越了直接微调，并在多样化的视觉中心任务上取得了最先进的成果——包括那些难以具体化中间抽象的任务，同时也能在多任务指令调优中实现泛化。",
    "url": "https://huggingface.co/papers/2512.21218",
    "arxiv_url": "https://arxiv.org/abs/2512.21218"
  },
  {
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.",
    "translation": "标题：自回归模型中的新兴时间抽象促进层次强化学习\n\n摘要：在基于下一个标记预测进行预训练并通过强化学习（RL）进行微调的大规模自回归模型取得了前所未有的成功，涵盖了许多问题领域。在强化学习过程中，这些模型通过逐个生成新的输出进行探索。然而，逐个标记采样行为可能导致学习效率低下，特别是在奖励稀疏的情况下。在此，我们展示了通过在自回归模型的内部表征中进行行动和探索可以克服这一问题。具体而言，为了发现时间抽象的行为，我们引入了一种高阶非因果序列模型，其输出控制基础自回归模型的残差流激活。在具有层次结构的网格世界和基于MuJoCo的任务中，我们发现高阶模型学会将长激活序列块压缩到内部控制器。关键在于，每个控制器执行一系列在长时间尺度上展开的具有行为意义的动作，并伴随学习到的终止条件，从而使得在时间上组合多个控制器能够在新任务上实现高效的探索。我们表明，直接的内部控制器强化过程，即我们称之为“内部强化学习”，使得在标准强化学习微调失败的情况下能够从稀疏奖励中学习。我们的结果证明了在自回归模型中潜在动作生成和强化学习的好处，建议内部强化学习作为在基础模型中实现层次强化学习的有希望的途径。",
    "url": "https://huggingface.co/papers/2512.20605",
    "arxiv_url": "https://arxiv.org/abs/2512.20605"
  },
  {
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
    "translation": "标题：Spatia：具有可更新空间记忆的视频生成\n\n摘要：现有的视频生成模型在保持长期空间和时间一致性方面面临困难，这主要是由于视频信号的稠密和高维特性。为了解决这一限制，我们提出了Spatia，一个关注空间记忆的视频生成框架，该框架明确地将三维场景点云视为持久的空间记忆。Spatia通过依赖于这一空间记忆迭代生成视频片段，并通过视觉惯性导航（SLAM）不断更新此记忆。这种动态-静态解耦设计在整个生成过程中增强了空间一致性，同时保留了模型生成真实动态实体的能力。此外，Spatia还支持显式相机控制和三维感知交互编辑等应用，为可扩展的、基于记忆的视频生成提供了几何基础框架。",
    "url": "https://huggingface.co/papers/2512.15716",
    "arxiv_url": "https://arxiv.org/abs/2512.15716"
  },
  {
    "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
    "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.",
    "translation": "标题：舍恩费尔德的语言模型数学推理解剖学\n\n摘要：大型语言模型越来越多地展现出推理轨迹，但其潜在的认知结构和步骤仍然难以识别和分析，超出了表层统计的范围。我们采用舍恩费尔德的情节理论作为一种归纳的、中等规模的视角，并引入ThinkARM（模型推理的解剖学），这是一种可扩展框架，能够将推理轨迹明确地抽象为功能性推理步骤，如分析、探索、实现、验证等。应用于多种模型的数学问题解决时，这种抽象揭示了可重复的思维动态以及推理模型与非推理模型之间的结构差异，而这些在符号级别的视角中并不明显。我们进一步提供了两项诊断案例研究，显示探索作为一个与正确性相关的关键分支步骤，其效率导向的方法选择性地抑制评估反馈步骤，而非统一缩短回应时间。总之，我们的研究结果表明，情节级别的表示使推理步骤明确化，从而能够系统地分析现代语言模型中推理是如何构建、稳定和改变的。",
    "url": "https://huggingface.co/papers/2512.19995",
    "arxiv_url": "https://arxiv.org/abs/2512.19995"
  },
  {
    "title": "How Much 3D Do Video Foundation Models Encode?",
    "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
    "translation": "标题：视频基础模型在多大程度上编码三维信息？\n\n摘要：视频是三维世界的连续二维投影。在对大规模视频数据进行训练后，全球的三维理解是否会自然而然地出现？我们通过量化现有视频基础模型（VidFMs）在海量视频数据上预训练后的三维理解能力来研究这个问题。我们提出了第一个模型无关的框架，通过浅层读取方式从特征中估计多个三维属性来衡量各种VidFMs的三维意识。我们的研究在多个维度上提供了有关VidFMs三维意识的有意义发现。特别是，我们展示了最先进的视频生成模型尽管没有在任何三维数据上进行训练，却展现出对三维物体和场景的强烈理解。这种理解甚至可以超过专门为三维任务训练的大型专家模型。我们的发现连同主要VidFMs的三维基准测试，为构建可扩展的三维模型提供了宝贵的观察。",
    "url": "https://huggingface.co/papers/2512.19949",
    "arxiv_url": "https://arxiv.org/abs/2512.19949"
  },
  {
    "title": "VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
    "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-π, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-π formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-π introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-π enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.",
    "translation": "标题：VA-π：针对像素感知自回归生成的变分策略对齐\n\n摘要：自回归（AR）视觉生成依赖于令牌器将图像映射至离散序列。然而，令牌器的训练目标是从真实令牌重构干净的图像，而AR生成器仅针对令牌的似然性进行优化。这种不对齐导致生成的令牌序列可能解码成低质量图像，且缺乏来自像素空间的直接监督。我们提出了VA-π，一个轻量级的后期训练框架，旨在通过一个有原则的像素空间目标来直接优化AR模型。VA-π将生成器与令牌器的对齐形式化为变分优化，推导出一个证据下界（ELBO），统一了像素重构和自回归建模。为了在离散令牌空间中进行优化，VA-π引入了一种基于强化学习的对齐策略，将AR生成器视为一种策略，并使用像素空间的重构质量作为其内在奖励。奖励通过预测的令牌序列在教师引导下重构原始图像的效果来衡量，从而为模型提供像素级的直接指导，而无需昂贵的自由运行采样。ELBO的正则化项作为一种自然的正则化器，保持令牌的分布一致性。VA-π使现有AR生成器能够快速适应，无需重新训练令牌器或外部奖励模型。在仅使用1%的ImageNet-1K数据和25分钟的微调后，FID从14.36降低到7.65，IS从86.55提高到116.70，同时在GenEval的文本到图像任务中，对于视觉生成模型（LlamaGen：从0.306提升至0.339）和统一多模态模型（Janus-Pro：从0.725提升至0.744）也取得了显著的进展。代码可在https://github.com/Lil-Shake/VA-Pi获取。",
    "url": "https://huggingface.co/papers/2512.19680",
    "arxiv_url": "https://arxiv.org/abs/2512.19680"
  },
  {
    "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
    "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.",
    "translation": "标题：GTR-Turbo：合并检查点秘密地成为代理性视觉语言模型训练的“免费”教师\n\n摘要：基于视觉语言模型（VLM）的多模态智能体的多回合强化学习（RL）受到稀疏奖励和长期信用分配的限制。近期的方法通过查询提供逐步反馈的教师（例如，导向思维增强（GTR）和在政策蒸馏），增加奖励密度，但通常依赖成本高昂且往往是特权模型作为教师，这限制了其实用性和可重复性。我们提出了GTR-Turbo，这是一种GTR的高效升级，它在无需训练或查询昂贵教师模型的情况下，达到相同的性能。具体而言，GTR-Turbo合并了在进行中的RL训练中生成的检查点的权重，然后使用该合并模型作为一个“免费”教师，通过监督微调或软逻辑蒸馏来指导后续的RL。这一设计消除了对特权VLM（例如，GPT或Gemini）的依赖，减轻了先前研究中观察到的“熵崩溃”现象，并保持训练的稳定性。在各种视觉代理任务中，GTR-Turbo将基线模型的准确性提高了10-30%，同时将壁钟训练时间减少了50%，计算成本相较于GTR降低了60%。",
    "url": "https://huggingface.co/papers/2512.13043",
    "arxiv_url": "https://arxiv.org/abs/2512.13043"
  }
]