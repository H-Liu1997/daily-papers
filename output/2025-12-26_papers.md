# HF Daily Papers - 2025-12-26

Total: 3 papers

---

## 1. Latent Implicit Visual Reasoning

**Upvotes**: 51 | **Authors**: Kelvin Li, Chuyi Shang, Leonid Karlinsky...

[HuggingFace](https://huggingface.co/papers/2512.21218) | [arXiv](https://arxiv.org/abs/2512.21218)

**输入→输出**: 任务类型: 视觉推理。输入: 图像和文本指令。输出: 答案或生成的图像。Backbone: 大规模多模态模型(LMM)。数据集: 多种视觉推理数据集。

**问题**: 现有的大规模多模态模型(LMMs)主要以文本为中心进行推理，在处理以视觉为主的任务时能力受限。之前的解决方法通常需要使用辅助图像、深度图或图像裁剪等方式来监督中间视觉步骤，但这些方法增加了标注成本，并且对“有用的”视觉抽象施加了限制性先验，难以跨任务泛化。

**方案**: 1. 架构: 提出了一种任务无关的机制，允许LMMs发现和使用视觉推理token。这些token以任务自适应的方式全局关注并重新编码图像。2. 训练方法: 通过训练LMMs来学习无监督的视觉推理token，无需手动标注中间步骤。3. 关键创新: 使用隐式视觉推理token，避免了对中间视觉抽象的手动指定，实现了任务无关的视觉推理能力。4. 结果: 在各种以视觉为中心的任务上，该方法优于直接微调，并取得了state-of-the-art的结果，并且可以泛化到多任务指令调整。

<details>
<summary>Original Abstract</summary>

While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>

---

## 2. Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

**Upvotes**: 46 | **Authors**: Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel...

[HuggingFace](https://huggingface.co/papers/2512.20605) | [arXiv](https://arxiv.org/abs/2512.20605)

**输入→输出**: 任务类型(生成)。输入: 文本token序列。输出: 文本token序列。Backbone: 自回归模型。

**问题**: 大型自回归模型在强化学习中虽然取得了成功，但逐token生成动作导致学习效率低下，尤其是在奖励稀疏的环境中。现有的强化学习方法难以有效地利用自回归模型的内部表征进行探索和抽象动作的学习。

**方案**: 1. 架构: 引入了一个高阶非因果序列模型，该模型控制基础自回归模型的残差流激活。2. 训练方法: 使用“内部强化学习 (Internal RL)”直接强化高阶模型生成的内部控制器。3. 关键创新: 高阶模型学习将长的激活序列块压缩到内部控制器中，每个控制器执行一系列有意义的行为动作，这些动作在长时间尺度上展开，并伴随一个学习到的终止条件。多个控制器可以组合，实现对新任务的有效探索。4. 结果: 在网格世界和 MuJoCo 任务中，证明了该方法能够从稀疏奖励中学习，而标准的强化学习微调失败。证明了内部控制器的强化能够提升学习效率。

<details>
<summary>Original Abstract</summary>

Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>

---

## 3. Spatia: Video Generation with Updatable Spatial Memory

**Upvotes**: 20 | **Authors**: Jinjing Zhao, Fangyun Wei, Zhening Liu...

[HuggingFace](https://huggingface.co/papers/2512.15716) | [arXiv](https://arxiv.org/abs/2512.15716)

**输入→输出**: 任务类型: 视频生成。输入: 文本描述/初始图像, 空间记忆。输出: 视频片段。Backbone: 未明确说明。数据集: 未明确说明。

**问题**: 现有视频生成模型难以保持长时间的空间和时间一致性，因为视频信号具有密集和高维的特性。之前的模型在生成过程中难以维持场景中物体的身份和位置，导致视频出现空间扭曲和不连贯。

**方案**: 1. 架构: Spatia，一种空间记忆感知的视频生成框架，包含一个3D场景点云作为持久的空间记忆。模型迭代地生成以空间记忆为条件的视频片段，并通过视觉SLAM持续更新空间记忆。2. 训练方法: 未明确说明，但暗示了使用空间记忆和视觉SLAM进行训练。3. 关键创新: 动态-静态解耦设计，通过空间记忆显式地保持3D场景信息，从而提高空间一致性，同时保留模型生成真实动态实体的能力。4. 结果: 论文声称Spatia能够实现显式的相机控制和3D感知的交互式编辑，没有给出具体性能指标，但强调了其可扩展性和基于记忆的视频生成能力。

<details>
<summary>Original Abstract</summary>

Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.

</details>

---

