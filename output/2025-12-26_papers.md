# HF Daily Papers - 2025-12-26

Total: 3 papers

---

## 1. Latent Implicit Visual Reasoning

**Upvotes**: 51 | **Authors**: Kelvin Li, Chuyi Shang, Leonid Karlinsky...

[HuggingFace](https://huggingface.co/papers/2512.21218) | [arXiv](https://arxiv.org/abs/2512.21218)

**输入→输出**: 任务: 视觉推理。输入: 图像和文本提示。输出: 文本答案。Backbone: Qwen2.5-VL-3B-Instruct, Qwen3-VL-4B-Instruct, LLaVA-OneVision-1.5-4B-Instruct。数据集: PixMo-Count, COCO, ArtBench-10, SPair-71k, HPatches, FunK-Point, MID, DreamSim。

**问题**: 以往的大型多模态模型 (LMM) 过分依赖于语言作为推理模态，因此在处理主要依赖视觉的任务时能力不足。通过监督中间视觉步骤（例如辅助图像，深度图或图像裁剪）来训练LMM的现有方法对视觉抽象施加了限制性的先验，增加了标注成本，并且难以跨任务泛化。

**方案**: 1. 模型结构: 在LMM中加入K个潜在视觉token，并在视觉输入和答案token之间使用 bottleneck 注意力机制，限制答案token只能通过潜在token关注视觉信息。2. 训练: 使用负对数似然 (NLL) 损失进行两阶段训练。第一阶段训练潜在token来捕获视觉信息，第二阶段允许答案token关注原始图像token和潜在token。使用LoRA调整语言backbone，冻结视觉编码器和投影层，仅解冻潜在token对应的嵌入层。使用AdamW优化器，学习率1e-4，权重衰减0.01， betas为(0.9, 0.999)，epsilon为1e-8，per-device batch size为1，梯度累积步数为8。Stage1训练4个epoch, Stage2训练6个epoch。3. 推理流程: 在推理时，将训练好的潜在token附加到提示中，并按照标准的LMM推理流程生成文本答案。4. 实验结果: 在九个视觉任务上，LIVR始终优于直接监督微调 (Direct SFT)，平均提升6.24% (Qwen2.5-VL)，3.43% (Qwen3-VL)和5.60% (LLaVA-OneVision)。在多任务训练中，LIVR在所有任务上都优于Direct SFT。

<details>
<summary>Original Abstract</summary>

While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>

---

## 2. Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

**Upvotes**: 46 | **Authors**: Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel...

[HuggingFace](https://huggingface.co/papers/2512.20605) | [arXiv](https://arxiv.org/abs/2512.20605)

**输入→输出**: 任务：学习分层强化学习策略。输入：原始观测。输出：抽象动作。Backbone：Transformer和状态空间模型(SSM)。数据集：自定义分层强化学习环境。

**问题**: 仅依赖逐个token变化进行探索可能不足以在需要正确生成多个token才能获得奖励的困难稀疏奖励问题上取得进展。

**方案**: 1. 模型结构：使用元控制器控制基础自回归模型的残差流激活。元控制器由编码器-解码器结构的生成式随机循环神经网络构成，包括用于时间整合的开关单元。编码器是非因果的，解码器产生线性控制器。2. 训练：使用变分推断算法进行自监督学习，优化包括低级动作预测和先验匹配正则化的损失函数。元控制器参数通过最小化损失函数进行训练，同时保持基础模型固定。3. 推理流程：元控制器生成内部控制器，用于指导基础模型的行为。通过内部强化学习，直接在残差流中进行强化学习，将内部激活视为观测，将元控制器输出视为动作。4. 实验结果：在网格世界和MuJoCo环境中，内部强化学习显著优于标准强化学习微调和分层强化学习方法CompILE。

<details>
<summary>Original Abstract</summary>

Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>

---

## 3. Spatia: Video Generation with Updatable Spatial Memory

**Upvotes**: 20 | **Authors**: Jinjing Zhao, Fangyun Wei, Zhening Liu...

[HuggingFace](https://huggingface.co/papers/2512.15716) | [arXiv](https://arxiv.org/abs/2512.15716)

**输入→输出**: 任务: 文本/图像到视频生成。输入: 文本指令、初始图像。输出: 视频片段。Backbone: Wan2.2 (包含5B参数)。数据集: RealEstate(40K视频), SpatialVID(HD子集，10K视频)。

**问题**: 现有的视频生成模型难以保持长期的空间和时间一致性，这是因为视频信号的密集和高维度特性。

**方案**: 1. 模型结构: 包含8个网络块，每个网络块包含一个ControlNet块，与四个主块并行运行。主块遵循Wan2.2的设计，包含一个自注意力层、一个交叉注意力层和一个FFN。ControlNet块附加一个投影器，实现为一个简单的MLP层。Flow Matching用于模型训练。
2. 训练: ControlNet块首先训练8000次迭代，冻结主网络。然后，冻结ControlNet块，并使用LoRA(rank=64)对主块进行微调，迭代5000次。AdamW优化器，学习率分别为le-5和1e-4，batch size为64，在64x AMD MI250 GPU上进行。默认情况下，该模型为第一次（图像条件）迭代生成81帧，为后续（片段条件）迭代生成72帧，在9个先前生成的帧上进行条件设置。
3. 推理流程: 用户指定文本指令和基于当前3D场景点云的相机轨迹，以生成新的视频片段。新生成的内容，连同先前生成的片段，然后用于更新空间记忆（场景点云）。
4. 实验结果: 在WorldScore基准测试中，Spatia优于其他静态场景生成模型和基础视频生成模型。在RealEstate数据集上，Spatia的PSNR为18.58，SSIM为0.646，LPIPS为0.254。

<details>
<summary>Original Abstract</summary>

Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.

</details>

---

